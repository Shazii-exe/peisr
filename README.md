# PEISR â€“ Prompt Enhancement & Intelligent Self-Refinement

PEISR is a research-oriented Streamlit application designed to study and evaluate
**prompt enhancement techniques** using rewriting, judging, and human-in-the-loop feedback.

The system enables controlled A/B comparisons between original and enhanced prompts,
while logging model behavior and human ratings for analysis.

---

## âœ¨ Features

- Prompt rewriting using LLMs (Gemini)
- Automated judging of responses
- Human rating interface (blind to judge output)
- Admin-only judge JSON visibility
- A/B testing support
- SQLite logging for experiments
- Streamlit-based interactive UI

---

## ğŸ§  Research Motivation

Prompt engineering often lacks structured evaluation.
PEISR introduces:
- Separation between **public raters** and **system judges**
- Controlled visibility to prevent bias
- Persistent logging for experimental analysis

This makes PEISR suitable for:
- Academic research
- Prompt evaluation studies
- Early-stage benchmarking of LLM behaviors

---

## ğŸ› ï¸ Tech Stack

- Python 3.10+
- Streamlit
- Google Gemini API
- SQLite
- Prompt engineering & evaluation logic

---

## ğŸ“‚ Project Structure

```text
peisr/
â”œâ”€â”€ app.py                  # Streamlit app entry point
â”œâ”€â”€ rewriter.py             # Prompt rewriting logic
â”œâ”€â”€ judge.py                # Automated judging logic
â”œâ”€â”€ prompts.py              # Prompt templates
â”œâ”€â”€ gemini_client.py        # Gemini API wrapper
â”œâ”€â”€ db.py                   # SQLite logging
â”œâ”€â”€ experiment_runner.py    # Offline experiments
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
